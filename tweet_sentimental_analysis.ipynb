{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the required libraries.\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport re\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nfrom nltk.stem.porter import *\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-24T20:46:03.405781Z","iopub.execute_input":"2022-01-24T20:46:03.406474Z","iopub.status.idle":"2022-01-24T20:46:08.885276Z","shell.execute_reply.started":"2022-01-24T20:46:03.406350Z","shell.execute_reply":"2022-01-24T20:46:08.884521Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Rename the column names and read the data.\ncolumns = ['TweetID','Entity', 'Output', 'Tweet']\ndata = pd.read_csv(r'../input/twitter-entity-sentiment-analysis/twitter_validation.csv', names=columns, header = None)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:08.888107Z","iopub.execute_input":"2022-01-24T20:46:08.888694Z","iopub.status.idle":"2022-01-24T20:46:08.910129Z","shell.execute_reply.started":"2022-01-24T20:46:08.888651Z","shell.execute_reply":"2022-01-24T20:46:08.909378Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Printing the first five  rows of the data.\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:08.911227Z","iopub.execute_input":"2022-01-24T20:46:08.911531Z","iopub.status.idle":"2022-01-24T20:46:08.931866Z","shell.execute_reply.started":"2022-01-24T20:46:08.911496Z","shell.execute_reply":"2022-01-24T20:46:08.931108Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#For storing clean data.\nmain =[]\n\n# Storing puntuations and special characters like(-, /, $) using re library.\na = re.compile('[%s]' % re.escape(string.punctuation ))\n\n# Storing stopword such as (a, the, is, etc) using nltk library.\nstop_words = set(stopwords.words('english'))\n\n# Making an object for Stemmer and Lemmatization using nlkt library.\np_stem = PorterStemmer()\nlem = WordNetLemmatizer()\n\n# Iterating complete data using tqdm library.\nfor i in tqdm(range(len(data['Tweet']))):\n    # Tokenization\n    token = word_tokenize(str(data['Tweet'][i]))\n    # Converting all data into lower case.\n    token = [w.lower() for w in token ]\n    # Removing all puntuations and special characters.\n    token = [a.sub('',w) for w in token]\n    # Removing all alpha-numeric from data.\n    token = [w for w in token if w.isalpha()]\n    # Removing all stopword from data.\n    b = [w for w in token if w not in stop_words]\n    # Stemming\n    b = [p_stem.stem(w) for w in b]\n    # Lemmatization\n    b = [lem.lemmatize(w) for w in b]\n    # Joining final data into a string.\n    c = ' '.join(b)\n    # Appending final data into main list.\n    main.append(c)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:08.933923Z","iopub.execute_input":"2022-01-24T20:46:08.934236Z","iopub.status.idle":"2022-01-24T20:46:11.482116Z","shell.execute_reply.started":"2022-01-24T20:46:08.934201Z","shell.execute_reply":"2022-01-24T20:46:11.481310Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Assigning main to the data\ndata['processed_Tweet'] = main\n# Deleting the Tweet column from data.\ndata = data.drop('Tweet', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:11.483422Z","iopub.execute_input":"2022-01-24T20:46:11.483848Z","iopub.status.idle":"2022-01-24T20:46:11.493536Z","shell.execute_reply.started":"2022-01-24T20:46:11.483797Z","shell.execute_reply":"2022-01-24T20:46:11.492671Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:11.494976Z","iopub.execute_input":"2022-01-24T20:46:11.495326Z","iopub.status.idle":"2022-01-24T20:46:11.510554Z","shell.execute_reply.started":"2022-01-24T20:46:11.495213Z","shell.execute_reply":"2022-01-24T20:46:11.509818Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Converting text into vector using sklearn library.\ncnt = CountVectorizer()\n# Converting vector into array.\nX = cnt.fit_transform(main).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:11.511708Z","iopub.execute_input":"2022-01-24T20:46:11.512314Z","iopub.status.idle":"2022-01-24T20:46:11.559968Z","shell.execute_reply.started":"2022-01-24T20:46:11.512278Z","shell.execute_reply":"2022-01-24T20:46:11.559310Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Printing all unique outputs.\ndata['Output'].unique()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:11.561258Z","iopub.execute_input":"2022-01-24T20:46:11.561724Z","iopub.status.idle":"2022-01-24T20:46:11.567676Z","shell.execute_reply.started":"2022-01-24T20:46:11.561688Z","shell.execute_reply":"2022-01-24T20:46:11.566892Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# As output has four label, so we converted them into binary using OneHotEncoder. \nenc =  OneHotEncoder(handle_unknown='ignore')\nd = np.array(data['Output'])\nd = d.reshape(-1,1)\ny = enc.fit_transform(d).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:11.569271Z","iopub.execute_input":"2022-01-24T20:46:11.569876Z","iopub.status.idle":"2022-01-24T20:46:11.576619Z","shell.execute_reply.started":"2022-01-24T20:46:11.569809Z","shell.execute_reply":"2022-01-24T20:46:11.575918Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into train and test.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42) ","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:11.579578Z","iopub.execute_input":"2022-01-24T20:46:11.580185Z","iopub.status.idle":"2022-01-24T20:46:11.597954Z","shell.execute_reply.started":"2022-01-24T20:46:11.580152Z","shell.execute_reply":"2022-01-24T20:46:11.597293Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:11.599033Z","iopub.execute_input":"2022-01-24T20:46:11.599299Z","iopub.status.idle":"2022-01-24T20:46:11.605156Z","shell.execute_reply.started":"2022-01-24T20:46:11.599263Z","shell.execute_reply":"2022-01-24T20:46:11.604370Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-01-24T20:46:11.606414Z","iopub.execute_input":"2022-01-24T20:46:11.607290Z","iopub.status.idle":"2022-01-24T20:46:11.614281Z","shell.execute_reply.started":"2022-01-24T20:46:11.607237Z","shell.execute_reply":"2022-01-24T20:46:11.613319Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}